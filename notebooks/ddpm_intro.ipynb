{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4f2046",
   "metadata": {},
   "source": [
    "# Denoising diffusion\n",
    "\n",
    "A brief introduction to generative diffusion modeling is provided in this notebook. In particular, the discussion focuses on the **denoising diffusion probabilistic model** (DDPM) [[Sohl-Dickstein et al., 2015](http://proceedings.mlr.press/v37/sohl-dickstein15.html), [Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)]. The relation to other generative modeling approaches such as **energy-based models** (EBMs), **variational autoencoders** (VAEs) or **normalizing flows** is emphasized in various review papers [[Bond-Taylor et al., 2022](https://ieeexplore.ieee.org/document/9555209), [Luo, 2022](https://arxiv.org/abs/2208.11970)]. Excellent explanations can also be found in the two blog posts [[Lilian Weng, 2021](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/), [Angus Turner, 2021](https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe685b27",
   "metadata": {},
   "source": [
    "## Forward and reverse diffusion\n",
    "\n",
    "A generative diffusion model usually consists of two processes. They transform between pure noise and data samples from the target distribution in a random fashion. The **forward diffusion** process gradually corrupts data by injecting noise. It is a modeled as a Markov chain with transition kernel $q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})$. Conditioned on a given a sample $\\boldsymbol{x}_0$, the density can be written as\n",
    "$$\n",
    "q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) =\n",
    "\\prod_{t=1}^T q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}).\n",
    "$$\n",
    "Including the unknown data distribution of $\\boldsymbol{X}_0 \\sim q(\\boldsymbol{x}_0)$, the joint density is $q(\\boldsymbol{x}_{0:T}) = q(\\boldsymbol{x}_0) \\prod_{t=1}^T q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})$.\n",
    "\n",
    "Vice versa, the **reverse diffusion** process gradually denoises unstructured noise from a fixed prior distribution $p(\\boldsymbol{x}_T)$ into a data samples. It is a learnable Markov chain that evolves backwards in time. Its transition kernels $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t)$ are parametrized by trainable parameters $\\boldsymbol{\\theta}$. The joint density of the reverse process is given as\n",
    "$$\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T}) =\n",
    "p(\\boldsymbol{x}_T) \\prod_{t=1}^T p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t).\n",
    "$$\n",
    "Integrating this density over the latent variables yields the marginal of a data point $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0) = \\int p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T}) \\, \\mathrm{d} \\boldsymbol{x}_{1:T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42063c76",
   "metadata": {},
   "source": [
    "## Training objective\n",
    "\n",
    "Unfortunately, the latter density $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)$ does not lend itself to **maximum likelihood estimation** (MLE) for finding $\\hat{\\boldsymbol{\\theta}} = \\mathrm{argmax}_\\boldsymbol{\\theta} \\, \\sum_{i=1}^N \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0,i})$ directly. The integral cannot be evaluated easily and is therefore intractable. For example, Monte Carlo simulation would suffer from a high variance which makes it very inefficient. Here, most samples generated from the prior would feature a low likelihood value.\n",
    "\n",
    "One can, however, derive and optimize a variational bound of the likelihood. In [[Sohl-Dickstein et al., 2015](http://proceedings.mlr.press/v37/sohl-dickstein15.html)], based on Jensen's inequality, such a bound $L$ with $\\mathbb{E}_{q(\\boldsymbol{x}_0)}[-\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)] \\leq L$ has been indeed established by\n",
    "$$\n",
    "L = \\mathbb{E}_{q(\\boldsymbol{x}_{0:T})} \\left[ - \\log \\left(\n",
    "\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})}\n",
    "{q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)} \\right) \\right] =\n",
    "\\mathbb{E}_{q(\\boldsymbol{x}_{0:T})} \\left[ - \\log p(\\boldsymbol{x}_T) - \\sum_{t=1}^T \\log \\left(\n",
    "\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t)}\n",
    "{q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})} \\right) \\right].\n",
    "$$\n",
    "\n",
    "The DDPM training task can then be formulated as the optimization problem $\\hat{\\boldsymbol{\\theta}} = \\mathrm{argmin}_\\boldsymbol{\\theta} \\, L$. Instead of minimizing the intractable negative log-likelihood, an upper bound of it is minimized.\n",
    "\n",
    "Exploiting the Markov property and Bayes' theorem in the form of $q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}) = q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}, \\boldsymbol{x}_0) = \\frac{q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0) \\, q(\\boldsymbol{x}_t | \\boldsymbol{x}_0)}{q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_0)}$, one can rewrite the variational bound in a more interpretable and even better computable way:\n",
    "$$\n",
    "L = \\mathbb{E}_{q(\\boldsymbol{x}_{0:T})} \\Bigg[\n",
    "\\underbrace{D_\\mathrm{KL}(q(\\boldsymbol{x}_T | \\boldsymbol{x}_0) \\, \\| \\, p(\\boldsymbol{x}_T))}_{L_T} +\n",
    "\\sum_{t=2}^T \\underbrace{D_\\mathrm{KL}(q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0) \\, \\| \\,\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t))}_{L_{t-1}}\n",
    "\\underbrace{-\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0 | \\boldsymbol{x}_1)}_{L_0} \\Bigg].\n",
    "$$\n",
    "The KL divergence $L_T$ quantifies how different the complete diffusion process $q(\\boldsymbol{x}_T | \\boldsymbol{x}_0)$ is from the pure noise prior $p(\\boldsymbol{x}_T)$. As long as the forward process is not learnable, this term does not depend on $\\boldsymbol{\\theta}$ and can thus be neglected. The terms $L_1, \\ldots, L_{T-1}$ penalize the difference between $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t)$ and the posterior of the diffusion process $q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0)$. They can be seen as a kind of consistency loss. The remaining $L_0$ is a reconstruction-like loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340505c8",
   "metadata": {},
   "source": [
    "## Relation to VAEs\n",
    "\n",
    "Note that $L_0$ and $L_T$ are loss terms that are also encountered for a VAE. It therefore seems instructive to investigate the connection to VAEs more closely at this point. Let us consider the **Kullback-Leibler** (KL) **divergence** between the conditional distribution $q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)$ of the forward diffusion process and the posterior $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) = p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T}) / p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)$ of the reverse process:\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_\\mathrm{KL}(q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\, \\| \\,\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)) &=\n",
    "\\int q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\log \\left(\n",
    "\\frac{q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)}\n",
    "{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)}\n",
    "\\right) \\, \\mathrm{d} \\boldsymbol{x}_{1:T} =\n",
    "\\int q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\log \\left(\n",
    "\\frac{\\prod_{t=1}^T q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})}\n",
    "{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T}) / p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)}\n",
    "\\right) \\, \\mathrm{d} \\boldsymbol{x}_{1:T} \\\\ &=\n",
    "\\int q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\left(\n",
    "\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0) - \\log p(\\boldsymbol{x}_T) +\n",
    "\\sum_{t=1}^T \\log \\left( \\frac{q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})}\n",
    "{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t)} \\right)\n",
    "\\right) \\, \\mathrm{d} \\boldsymbol{x}_{1:T} \\\\ &=\n",
    "\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0) -\n",
    "\\mathbb{E}_{q(\\boldsymbol{x}_{1:T})} \\left[ \\log p(\\boldsymbol{x}_T) +\n",
    "\\sum_{t=1}^T \\log \\left( \\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t)}\n",
    "{q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1})} \\right) \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "By additionally averaging over the data distribution $q(\\boldsymbol{x}_0)$ one sees that $L = \\mathbb{E}_{q(\\boldsymbol{x}_0)}[-\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)] + \\mathbb{E}_{q(\\boldsymbol{x}_0)}[ D_\\mathrm{KL}(q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\, \\| \\, p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0))]$. The inequality from above then simply follows from $D_\\mathrm{KL}(q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\, \\| \\, p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0)) \\geq 0$. Moreoever, one can now argue that minimizing $L$ with respect to $\\boldsymbol{\\theta}$ amounts to maximizing $\\mathbb{E}_{q(\\boldsymbol{x}_0)}[\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)]$ and minimizing $\\mathbb{E}_{q(\\boldsymbol{x}_0)}[ D_\\mathrm{KL}(q(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0) \\, \\| \\, p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{1:T} | \\boldsymbol{x}_0))]$ at the same time. This is completely analogous to the VAE.\n",
    "\n",
    "Hence, a DDPM can be seen as a certain hierarchically defined VAE [[Luo, 2022](https://arxiv.org/abs/2208.11970)]. Both encoder and decoder have a Markovian structure. The encoder is predefined, instead of being learned from the data. It does not perform any dimension reduction and usually transforms to pure random noise. The DDPM latent space does therefore not play the exact same role as the latent VAE representation (mean or random sample of the learned posterior).\n",
    "\n",
    "As an aside, the expected value $\\mathbb{E}_{q(\\boldsymbol{x}_0)}[\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)]$ is normally used to show the equivalence of maximizing the log-likelihood and minimizing the KL divergence $D_\\mathrm{KL}(q(\\boldsymbol{x}_0) \\, \\| \\, p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)) = H[q(\\boldsymbol{x}_0)] - \\mathbb{E}_{q(\\boldsymbol{x}_0)}[\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)]$. Here, the approximation $\\mathbb{E}_{q(\\boldsymbol{x}_0)}[\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)] \\approx \\frac{1}{N} \\sum_{i=1}^N \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0,i})$ also clarifies the connection to the log-likelihood $\\sum_{i=1}^N \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0,i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba52db",
   "metadata": {},
   "source": [
    "## Normal distributions\n",
    "\n",
    "Both the forward and the reverse process can be constructed on the basis of Gaussian distributions. As usual, this greatly simplifies the analysis. As for the diffusion process, a natural choice for the Markov transition kernel is\n",
    "$$\n",
    "q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}) =\n",
    "\\mathcal{N} \\left( \\boldsymbol{x}_t | \\sqrt{1-\\beta_t} \\boldsymbol{x}_{t-1}, \\beta_t \\boldsymbol{I} \\right).\n",
    "$$\n",
    "Here, $\\beta_t \\in (0,1)$ specifies the noise variance. It can be set to a constant or one can assume a certain variance schedule. For example, increasing variances with $0 < \\beta_1 < \\ldots < \\beta_T < 1$ are a common specification. Either way, the density $q(\\boldsymbol{x}_t | \\boldsymbol{x}_0)$ for all $t \\in \\{1,\\ldots,T\\}$ can be calculated as\n",
    "$$\n",
    "q(\\boldsymbol{x}_t | \\boldsymbol{x}_0) =\n",
    "\\mathcal{N} \\left( \\boldsymbol{x}_t | \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0,\n",
    "(1-\\bar{\\alpha}_t) \\boldsymbol{I} \\right), \\quad\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s, \\quad \\alpha_t = 1-\\beta_t.\n",
    "$$\n",
    "For $\\bar{\\alpha}_T \\approx 0$ one can easily see that $q(\\boldsymbol{x}_T | \\boldsymbol{x}_0) \\approx \\mathcal{N}(\\boldsymbol{x}_T | \\boldsymbol{0}, \\boldsymbol{I})$ approaches a Gaussian with zero mean and unit variance. It is noteworthy that this distribution does not depend on the initial state $\\boldsymbol{x}_0$ any longer.\n",
    "\n",
    "Realizations of the Markov chain can be obtained by iteratively performing state updates. Here, the new state $\\boldsymbol{x}_t = \\sqrt{1-\\beta_t} \\boldsymbol{x}_{t-1} + \\sqrt{\\beta_t} \\boldsymbol{\\epsilon}$ emerges from the previous one $\\boldsymbol{x}_{t-1}$ by incorporating random noise $\\boldsymbol{\\epsilon}$ that is randomly sampled from a standard normal distribution $\\mathcal{N}(\\boldsymbol{\\epsilon} | \\boldsymbol{0}, \\boldsymbol{I})$.\n",
    "Similarly, one can simulate any state $\\boldsymbol{x}_t = \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}$ directly from the initial $\\boldsymbol{x}_0$ without having to perform all intermediate steps as above.\n",
    "\n",
    "Another appealing consequence of the Gaussian design choice is that the conditional distributions $q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0)$, which are occurring in the loss function above, are Gaussian as well. While the derivation is not important here, the result is provided for the sake of completeness:\n",
    "$$\n",
    "\\begin{align*}\n",
    "q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0) &=\n",
    "\\mathcal{N} \\left( \\boldsymbol{x}_{t-1} | \\tilde{\\boldsymbol{\\mu}}_t\n",
    "(\\boldsymbol{x}_t, \\boldsymbol{x}_0), \\tilde{\\beta}_t \\boldsymbol{I} \\right), \\\\\n",
    "\\tilde{\\boldsymbol{\\mu}}_t (\\boldsymbol{x}_t, \\boldsymbol{x}_0) &=\n",
    "\\frac{\\sqrt{\\alpha_t} (1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} \\boldsymbol{x}_t +\n",
    "\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1-\\bar{\\alpha}_t} \\boldsymbol{x}_0, \\\\\n",
    "\\tilde{\\beta}_t &= \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In view of the computation of $L_{t-1} = \\mathbb{E}_{q(\\boldsymbol{x}_0, \\boldsymbol{x}_t)}[D_\\mathrm{KL}(q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0) \\, \\| \\, p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t))]$, it is beneficial to also model the reverse process by Gaussian distributions. A standard normal prior $p(\\boldsymbol{x}_T) = \\mathcal{N}(\\boldsymbol{x}_T | \\boldsymbol{0}, \\boldsymbol{I})$ is usually considered together with learnable Gaussian transition densities\n",
    "$$\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t) =\n",
    "\\mathcal{N}(\\boldsymbol{x}_{t-1} | \\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t),\n",
    "\\sigma_t^2 \\boldsymbol{I}).\n",
    "$$\n",
    "The standard choice are fixed variances with either $\\sigma_t^2 = \\beta_t$ or $\\sigma_t^2 = \\tilde{\\beta}_t$, Of course, one could generalize this approach by assuming learnable variances or more complex covariance models $\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)$ [[Nichol and Dhariwal, 2021](https://proceedings.mlr.press/v139/nichol21a.html)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430b994",
   "metadata": {},
   "source": [
    "## Further simplifications\n",
    "\n",
    "The KL divergence between two multivariate Gaussian distributions is analytically available. This can be readily exploited in the computation of the loss terms $L_{t-1}$ for $t = 2,\\ldots,T$. For instance, with $\\sigma_t^2 = \\tilde{\\beta}_t$ one would have\n",
    "$$\n",
    "L_{t-1} = \\mathbb{E}_{q(\\boldsymbol{x}_0, \\boldsymbol{x}_t)}[\n",
    "D_\\mathrm{KL}(q(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t, \\boldsymbol{x}_0) \\, \\| \\,\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t))] =\n",
    "\\mathbb{E}_{q(\\boldsymbol{x}_0, \\boldsymbol{x}_t)} \\left[ \\frac{1}{2 \\sigma_t^2} \\left\\lVert\n",
    "\\tilde{\\boldsymbol{\\mu}}_t(\\boldsymbol{x}_t, \\boldsymbol{x}_0) -\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)\n",
    "\\right\\rVert^2\\right] + \\text{const.},\n",
    "$$\n",
    "where terms that do not depend on $\\boldsymbol{\\theta}$ have been omitted. By minimizing the mean squared error, neural network $\\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)$ can be trained to predict the posterior mean $\\tilde{\\boldsymbol{\\mu}}_t (\\boldsymbol{x}_t, \\boldsymbol{x}_0)$ of the forward process.\n",
    "\n",
    "A reparametrization of this model has been proposed in [[Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)]. Since $\\boldsymbol{x}_t = \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}$ can be used for computing realizations of the forward process, one can also write $\\boldsymbol{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (\\boldsymbol{x}_t - \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon})$. This can be directly plugged into the expression for $\\tilde{\\boldsymbol{\\mu}}_t (\\boldsymbol{x}_t, \\boldsymbol{x}_0)$ in order to obtain $\\tilde{\\boldsymbol{\\mu}}_t (\\boldsymbol{x}_t, \\boldsymbol{\\epsilon}) = \\frac{1}{\\sqrt{\\alpha_t}}(\\boldsymbol{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\boldsymbol{\\epsilon})$. Motivated by this form, one can parametrize the model as\n",
    "$$\n",
    "\\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t) =\n",
    "\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\boldsymbol{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\n",
    "\\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t) \\right).\n",
    "$$\n",
    "Instead of predicting the posterior mean, the newly introduced a model $\\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)$ predicts the noise $\\boldsymbol{\\epsilon}$ that is responsible for the transition from $\\boldsymbol{x}_0$ to $\\boldsymbol{x}_t = \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}$. The loss term for its training is given as\n",
    "$$\n",
    "L_{t-1} = \\mathbb{E}_{q(\\boldsymbol{x}_0), \\mathcal{N}(\\boldsymbol{\\epsilon} | \\boldsymbol{0}, \\boldsymbol{I})}\n",
    "\\left[ \\frac{\\beta_t^2}{2 \\sigma_t^2 \\alpha_t (1-\\bar{\\alpha}_t)}\n",
    "\\left\\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta} \\left(\n",
    "\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}, t \\right)\n",
    "\\right\\rVert^2 \\right].\n",
    "$$\n",
    "\n",
    "The remaining term $L_0 = \\mathbb{E}_{q(\\boldsymbol{x}_0, \\boldsymbol{x}_1)}[-\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0 | \\boldsymbol{x}_1)]$ with $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0 | \\boldsymbol{x}_1) = \\mathcal{N}(\\boldsymbol{x}_0 | \\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_1, 1), \\sigma_1^2 \\boldsymbol{I})$ can surprisingly, if discarding the normalization, be brought into the same form. An unweighted form of the training objective can then be written as\n",
    "$$\n",
    "L_\\text{simple} = \\mathbb{E}_{\\mathcal{U}(t|1, T), q(\\boldsymbol{x}_0),\n",
    "\\mathcal{N}(\\boldsymbol{\\epsilon} | \\boldsymbol{0}, \\boldsymbol{I})}\n",
    "\\left[ \\left\\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta} \\left(\n",
    "\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}, t \\right)\n",
    "\\right\\rVert^2 \\right].\n",
    "$$\n",
    "Here, $t$ is randomly distributed according to a uniform distribution $\\mathcal{U}(t|1, T)$.\n",
    "The parameters of the model $\\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t) = \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha_t}} \\boldsymbol{\\epsilon}, t)$ are usually shared over $t$. After all, this is a unexpectedly simple training objective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
